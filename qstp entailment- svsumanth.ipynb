{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df60cc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-gpu in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: keras~=2.6 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.6.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.39.0)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.6.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.17.3)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.19.5)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.7.4.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.12)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.4.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.15.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.13.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.12.1)\n",
      "Requirement already satisfied: clang~=5.0 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (5.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.1.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.6.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.3.0)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.36.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow-gpu) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow-gpu) (52.0.0.post20210125)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow-gpu) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow-gpu) (0.6.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow-gpu) (1.35.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow-gpu) (2.25.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow-gpu) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow-gpu) (0.4.5)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow-gpu) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow-gpu) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow-gpu) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow-gpu) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow-gpu) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu) (2020.12.5)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow-gpu) (3.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec68b510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (1.19.5)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a8ba8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (3.3.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from matplotlib) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\sv_su\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02ca7b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import urllib\n",
    "import sys\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "786efc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#used the global vectors for word representation (GloVe) which consists of the vector representations for words\n",
    "glove_zip_file = \"glove.6B.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e171333",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors_file = \"glove.6B.50d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f82c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#excracting and unzipping the glove and SNLI zip files\n",
    "from urllib.request import urlretrieve\n",
    "if (not os.path.isfile(glove_zip_file) and\n",
    "    not os.path.isfile(glove_vectors_file)):\n",
    "    urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", glove_zip_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34f623e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_zip_file = \"snli_1.0.zip\"\n",
    "snli_dev_file = \"snli_1.0_dev.txt\"\n",
    "snli_full_dataset_file = \"snli_1.0_train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fc0b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not os.path.isfile(snli_zip_file) and\n",
    "    not os.path.isfile(snli_dev_file)):\n",
    "    urlretrieve (\"https://nlp.stanford.edu/projects/snli/snli_1.0.zip\", snli_zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "294c1336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating the words into a dictonary\n",
    "def unzip_single_file(zip_file_name, output_file_name):\n",
    "    if not os.path.isfile(output_file_name):\n",
    "        with open(output_file_name, 'wb') as out_file:\n",
    "            with zipfile.ZipFile(zip_file_name) as zipped:\n",
    "                for info in zipped.infolist():\n",
    "                    if output_file_name in info.filename:\n",
    "                        with zipped.open(info) as requested_file:\n",
    "                            out_file.write(requested_file.read())\n",
    "                            return\n",
    "\n",
    "unzip_single_file(glove_zip_file, glove_vectors_file)\n",
    "unzip_single_file(snli_zip_file, snli_dev_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "770e9e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "glove_wordmap = {}\n",
    "with open(glove_vectors_file, \"r\",encoding='utf-8') as glove:\n",
    "    for lines in glove:\n",
    "        name, vector = tuple(lines.split(\" \", 1))\n",
    "        glove_wordmap[name] = np.fromstring(vector, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc24b66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sentence2sequence(sentence):\n",
    "    tokens = sentence.lower().split(\" \")\n",
    "    rows = []\n",
    "    words = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        i = len(token)\n",
    "        while len(token) > 0 and i > 0:\n",
    "            word = token[:i]\n",
    "            if word in glove_wordmap:\n",
    "                rows.append(glove_wordmap[word])\n",
    "                words.append(word)\n",
    "                token = token[i:]\n",
    "                i = len(token)\n",
    "            else:\n",
    "                i = i-1\n",
    "    return rows, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55ac1e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM was used to make the model\n",
    "max_hypothesis_length, max_evidence_length = 30, 30\n",
    "batch_size, vector_size, hidden_size = 128, 50, 64\n",
    "\n",
    "lstm_size = hidden_size\n",
    "\n",
    "weight_decay = 0.0001\n",
    "\n",
    "learning_rate = 1\n",
    "\n",
    "input_p, output_p = 0.5, 0.5\n",
    "\n",
    "training_iterations_count = 100000\n",
    "\n",
    "display_step = 10\n",
    "\n",
    "def score_setup(row):\n",
    "    convert_dict = {\n",
    "      'entailment': 0,\n",
    "      'neutral': 1,\n",
    "      'contradiction': 2\n",
    "    }\n",
    "    score = np.zeros((3,))\n",
    "    for x in range(1,6):\n",
    "        tag = row[\"label\"+str(x)]\n",
    "        if tag in convert_dict: score[convert_dict[tag]] += 1\n",
    "    return score / (1.0*np.sum(score))\n",
    "\n",
    "def fit_to_size(matrix, shape):\n",
    "    res = np.zeros(shape)\n",
    "    slices = [slice(0,min(dim,shape[e])) for e, dim in enumerate(matrix.shape)]\n",
    "    res[slices] = matrix[slices]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d76d456c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-19b719728e41>:32: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  res[slices] = matrix[slices]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def split_data_into_scores():\n",
    "    import csv\n",
    "    with open(\"snli_1.0_dev.txt\",\"r\") as data:\n",
    "        train = csv.DictReader(data, delimiter='\\t')\n",
    "        evi_sentences = []\n",
    "        hyp_sentences = []\n",
    "        labels = []\n",
    "        scores = []\n",
    "        for row in train:\n",
    "            hyp_sentences.append(np.vstack(\n",
    "                    sentence2sequence(row[\"sentence1\"].lower())[0]))\n",
    "            evi_sentences.append(np.vstack(\n",
    "                    sentence2sequence(row[\"sentence2\"].lower())[0]))\n",
    "            labels.append(row[\"gold_label\"])\n",
    "            scores.append(score_setup(row))\n",
    "        \n",
    "        hyp_sentences = np.stack([fit_to_size(x, (max_hypothesis_length, vector_size))\n",
    "                          for x in hyp_sentences])\n",
    "        evi_sentences = np.stack([fit_to_size(x, (max_evidence_length, vector_size))\n",
    "                          for x in evi_sentences])\n",
    "                                 \n",
    "        return (hyp_sentences, evi_sentences), labels, np.array(scores)\n",
    "    \n",
    "data_feature_list, correct_values, correct_scores = split_data_into_scores()\n",
    "\n",
    "l_h, l_e = max_hypothesis_length, max_evidence_length\n",
    "N, D, H = batch_size, vector_size, hidden_size\n",
    "l_seq = l_h + l_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10e02196",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "lstm = tf.keras.layers.LSTMCell(lstm_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60b932be",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_drop =  tf.compat.v1.nn.rnn_cell.DropoutWrapper(lstm, input_p, output_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "75f84403",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "hyp = tf.compat.v1.placeholder(tf.float32, [N, l_h, D], 'hypothesis')\n",
    "evi = tf.compat.v1.placeholder(tf.float32, [N, l_e, D], 'evidence')\n",
    "y = tf.compat.v1.placeholder(tf.float32, [N, 3], 'label')\n",
    "lstm_back = tf.keras.layers.LSTMCell(lstm_size)\n",
    "lstm_drop_back = tf.compat.v1.nn.rnn_cell.DropoutWrapper(lstm_back, input_p, output_p)\n",
    "fc_initializer = tf.random_normal_initializer(stddev=0.1) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d249f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_weight = tf.compat.v1.get_variable('fc_weight', [2*hidden_size, 3], initializer = fc_initializer)\n",
    "fc_bias = tf.compat.v1.get_variable('bias', [3])\n",
    "tf.compat.v1.add_to_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES, tf.nn.l2_loss(fc_weight)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "71961ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = tf.concat([hyp, evi], 1) \n",
    "\n",
    "x = tf.transpose(x, [1, 0, 2]) \n",
    "\n",
    "x = tf.reshape(x, [-1, vector_size])\n",
    "\n",
    "x = tf.split(x, l_seq,)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "563837cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#note: this line does not work properly sometimes i could not figure out why. It works after a few attempts\n",
    "rnn_outputs, _, _= tf.compat.v1.nn.static_bidirectional_rnn(lstm, lstm_back, x,  dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "372c02b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_scores = tf.matmul(rnn_outputs[-1], fc_weight) + fc_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f29b217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.compat.v1.variable_scope('Accuracy'):\n",
    "    predicts = tf.cast(tf.argmax(classification_scores, 1), 'int32')\n",
    "    y_label = tf.cast(tf.argmax(y, 1), 'int32')\n",
    "    corrects = tf.equal(predicts, y_label)\n",
    "    num_corrects = tf.reduce_sum(tf.cast(corrects, tf.float32))\n",
    "    accuracy = tf.reduce_mean(tf.cast(corrects, tf.float32))\n",
    "\n",
    "with tf.compat.v1.variable_scope(\"loss\"):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits = classification_scores, labels = y)\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    total_loss = loss + weight_decay * tf.add_n(\n",
    "        tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES))\n",
    "\n",
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "opt_op = optimizer.minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c5c6cfb8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0.0, Minibatch Loss= 1.101009, Training Accuracy= 0.35156\n",
      "Iter 10.0, Minibatch Loss= 1.092380, Training Accuracy= 0.37500\n",
      "Iter 20.0, Minibatch Loss= 1.073564, Training Accuracy= 0.45312\n",
      "Iter 30.0, Minibatch Loss= 1.093903, Training Accuracy= 0.38281\n",
      "Iter 40.0, Minibatch Loss= 1.086954, Training Accuracy= 0.45312\n",
      "Iter 50.0, Minibatch Loss= 1.088913, Training Accuracy= 0.43750\n",
      "Iter 60.0, Minibatch Loss= 1.088047, Training Accuracy= 0.42969\n",
      "Iter 70.0, Minibatch Loss= 1.092216, Training Accuracy= 0.39844\n",
      "Iter 80.0, Minibatch Loss= 1.092379, Training Accuracy= 0.32812\n",
      "Iter 90.0, Minibatch Loss= 1.094183, Training Accuracy= 0.41406\n",
      "Iter 100.0, Minibatch Loss= 1.080356, Training Accuracy= 0.44531\n",
      "Iter 110.0, Minibatch Loss= 1.096282, Training Accuracy= 0.39844\n",
      "Iter 120.0, Minibatch Loss= 1.068137, Training Accuracy= 0.45312\n",
      "Iter 130.0, Minibatch Loss= 1.085275, Training Accuracy= 0.33594\n",
      "Iter 140.0, Minibatch Loss= 1.077732, Training Accuracy= 0.43750\n",
      "Iter 150.0, Minibatch Loss= 1.109047, Training Accuracy= 0.35938\n",
      "Iter 160.0, Minibatch Loss= 1.090351, Training Accuracy= 0.39062\n",
      "Iter 170.0, Minibatch Loss= 1.084727, Training Accuracy= 0.42969\n",
      "Iter 180.0, Minibatch Loss= 1.073277, Training Accuracy= 0.33594\n",
      "Iter 190.0, Minibatch Loss= 1.070912, Training Accuracy= 0.39062\n",
      "Iter 200.0, Minibatch Loss= 1.078712, Training Accuracy= 0.42969\n",
      "Iter 210.0, Minibatch Loss= 1.078212, Training Accuracy= 0.43750\n",
      "Iter 220.0, Minibatch Loss= 1.051827, Training Accuracy= 0.42969\n",
      "Iter 230.0, Minibatch Loss= 1.025962, Training Accuracy= 0.51562\n",
      "Iter 240.0, Minibatch Loss= 1.072716, Training Accuracy= 0.46094\n",
      "Iter 250.0, Minibatch Loss= 1.059359, Training Accuracy= 0.44531\n",
      "Iter 260.0, Minibatch Loss= 1.080503, Training Accuracy= 0.42969\n",
      "Iter 270.0, Minibatch Loss= 1.100458, Training Accuracy= 0.36719\n",
      "Iter 280.0, Minibatch Loss= 1.093296, Training Accuracy= 0.39844\n",
      "Iter 290.0, Minibatch Loss= 1.072396, Training Accuracy= 0.38281\n",
      "Iter 300.0, Minibatch Loss= 1.077955, Training Accuracy= 0.40625\n",
      "Iter 310.0, Minibatch Loss= 1.068567, Training Accuracy= 0.45312\n",
      "Iter 320.0, Minibatch Loss= 1.079836, Training Accuracy= 0.43750\n",
      "Iter 330.0, Minibatch Loss= 1.100486, Training Accuracy= 0.33594\n",
      "Iter 340.0, Minibatch Loss= 1.030258, Training Accuracy= 0.53125\n",
      "Iter 350.0, Minibatch Loss= 1.069829, Training Accuracy= 0.41406\n",
      "Iter 360.0, Minibatch Loss= 1.008463, Training Accuracy= 0.53125\n",
      "Iter 370.0, Minibatch Loss= 1.047738, Training Accuracy= 0.48438\n",
      "Iter 380.0, Minibatch Loss= 1.060491, Training Accuracy= 0.47656\n",
      "Iter 390.0, Minibatch Loss= 1.048012, Training Accuracy= 0.48438\n",
      "Iter 400.0, Minibatch Loss= 1.133728, Training Accuracy= 0.35938\n",
      "Iter 410.0, Minibatch Loss= 1.051010, Training Accuracy= 0.51562\n",
      "Iter 420.0, Minibatch Loss= 1.073701, Training Accuracy= 0.42969\n",
      "Iter 430.0, Minibatch Loss= 1.034814, Training Accuracy= 0.53906\n",
      "Iter 440.0, Minibatch Loss= 1.065718, Training Accuracy= 0.45312\n",
      "Iter 450.0, Minibatch Loss= 1.013028, Training Accuracy= 0.56250\n",
      "Iter 460.0, Minibatch Loss= 1.022369, Training Accuracy= 0.51562\n",
      "Iter 470.0, Minibatch Loss= 0.985612, Training Accuracy= 0.53125\n",
      "Iter 480.0, Minibatch Loss= 1.036344, Training Accuracy= 0.46875\n",
      "Iter 490.0, Minibatch Loss= 1.053864, Training Accuracy= 0.49219\n",
      "Iter 500.0, Minibatch Loss= 1.053780, Training Accuracy= 0.47656\n",
      "Iter 510.0, Minibatch Loss= 1.051512, Training Accuracy= 0.44531\n",
      "Iter 520.0, Minibatch Loss= 1.047961, Training Accuracy= 0.46875\n",
      "Iter 530.0, Minibatch Loss= 1.043157, Training Accuracy= 0.42969\n",
      "Iter 540.0, Minibatch Loss= 1.038031, Training Accuracy= 0.52344\n",
      "Iter 550.0, Minibatch Loss= 1.075225, Training Accuracy= 0.47656\n",
      "Iter 560.0, Minibatch Loss= 1.072380, Training Accuracy= 0.47656\n",
      "Iter 570.0, Minibatch Loss= 1.032564, Training Accuracy= 0.45312\n",
      "Iter 580.0, Minibatch Loss= 1.026903, Training Accuracy= 0.49219\n",
      "Iter 590.0, Minibatch Loss= 1.031234, Training Accuracy= 0.49219\n",
      "Iter 600.0, Minibatch Loss= 1.039389, Training Accuracy= 0.51562\n",
      "Iter 610.0, Minibatch Loss= 1.016813, Training Accuracy= 0.50781\n",
      "Iter 620.0, Minibatch Loss= 1.065514, Training Accuracy= 0.41406\n",
      "Iter 630.0, Minibatch Loss= 1.077271, Training Accuracy= 0.46875\n",
      "Iter 640.0, Minibatch Loss= 1.056644, Training Accuracy= 0.42969\n",
      "Iter 650.0, Minibatch Loss= 1.007223, Training Accuracy= 0.54688\n",
      "Iter 660.0, Minibatch Loss= 1.037466, Training Accuracy= 0.48438\n",
      "Iter 670.0, Minibatch Loss= 1.016987, Training Accuracy= 0.53906\n",
      "Iter 680.0, Minibatch Loss= 1.046870, Training Accuracy= 0.48438\n",
      "Iter 690.0, Minibatch Loss= 1.047536, Training Accuracy= 0.41406\n",
      "Iter 700.0, Minibatch Loss= 1.021933, Training Accuracy= 0.50781\n",
      "Iter 710.0, Minibatch Loss= 1.088305, Training Accuracy= 0.41406\n",
      "Iter 720.0, Minibatch Loss= 1.011570, Training Accuracy= 0.52344\n",
      "Iter 730.0, Minibatch Loss= 1.037966, Training Accuracy= 0.51562\n",
      "Iter 740.0, Minibatch Loss= 1.044008, Training Accuracy= 0.48438\n",
      "Iter 750.0, Minibatch Loss= 1.008456, Training Accuracy= 0.48438\n",
      "Iter 760.0, Minibatch Loss= 0.995477, Training Accuracy= 0.53906\n",
      "Iter 770.0, Minibatch Loss= 1.052699, Training Accuracy= 0.42188\n",
      "Iter 780.0, Minibatch Loss= 1.046170, Training Accuracy= 0.42188\n"
     ]
    }
   ],
   "source": [
    "init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "# Launching the Tensorflow session\n",
    "sess = tf.compat.v1.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# training_iterations_count: The number of data pieces to train on in total\n",
    "# batch_size: The number of data pieces per batch\n",
    "training_iterations = range(0,training_iterations_count,batch_size)\n",
    "\n",
    "\n",
    "for i in training_iterations:\n",
    "\n",
    "    \n",
    "    batch = np.random.randint(data_feature_list[0].shape[0], size=batch_size)\n",
    "    \n",
    "    hyps, evis, ys = (data_feature_list[0][batch,:],\n",
    "                      data_feature_list[1][batch,:],\n",
    "                      correct_scores[batch])\n",
    "    \n",
    "\n",
    "    sess.run([opt_op], feed_dict={hyp: hyps, evi: evis, y: ys})\n",
    "   \n",
    "    if (i/batch_size) % display_step == 0:\n",
    "        # Calculating batch accuracy\n",
    "        acc = sess.run(accuracy, feed_dict={hyp: hyps, evi: evis, y: ys})\n",
    "        # Calculating batch loss\n",
    "        tmp_loss = sess.run(loss, feed_dict={hyp: hyps, evi: evis, y: ys})\n",
    "        \n",
    "        print(\"Iter \" + str(i/batch_size) + \", Minibatch Loss= \" + \\\n",
    "              \"{:.6f}\".format(tmp_loss) + \", Training Accuracy= \" + \\\n",
    "              \"{:.5f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3e029bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the first statement: The boy likes playing sports \n"
     ]
    }
   ],
   "source": [
    "#inputting the initial statement\n",
    "first = input(\"Enter the first statement: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5149dd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The boy likes playing sports \n"
     ]
    }
   ],
   "source": [
    "print(first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0cc735d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the hypothesis: The boy is a singer\n"
     ]
    }
   ],
   "source": [
    "#inputting the initial statement\n",
    "hypo = input(\"Enter the hypothesis: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5725ac47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The boy likes playing sports ']\n",
      "['The boy is a singer']\n",
      "Contradiction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-19b719728e41>:32: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  res[slices] = matrix[slices]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s1 = [first]\n",
    "\n",
    "hypotheses = [hypo]\n",
    "\n",
    "#based on the prediction, gives entailment, contradiction or neutral\n",
    "sentence1 = [fit_to_size(np.vstack(sentence2sequence(evidence)[0]),\n",
    "                         (30, 50)) for evidence in s1]\n",
    "\n",
    "sentence2 = [fit_to_size(np.vstack(sentence2sequence(hypothesis)[0]),\n",
    "                         (30,50)) for hypothesis in hypotheses]\n",
    "\n",
    "prediction = sess.run(classification_scores, feed_dict={hyp: (sentence1 * N),\n",
    "                                                        evi: (sentence2 * N),\n",
    "                                                        y: [[0,0,0]]*N})\n",
    "print(s1)\n",
    "print(hypotheses)\n",
    "print([\"entailment\", \"Neutral\", \"Contradiction\"][np.argmax(prediction[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff35da2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
